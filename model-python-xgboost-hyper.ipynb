{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `XGBoost` 예측모형 자동화\n",
    "\n",
    "`XGBoost` 예측모형은 뛰어난 성능으로 이미 캐글 등 대다수 경진대회를 휩쓴 검증된 알고리즘이다. 그렇다고 모든 예측문제에 기계학습 알고리즘으로 선택을 할 수는 없다. 우선, 뛰어난 성능을 가진 XGBoost 알고리즘의 마지막까지 높일 수 있는 초모수(Hyper Paramter) 튜닝에 대해서 살펴보자.\n",
    "\n",
    "<img src=\"fig/xgboost-python.png\" alt=\"XGBoost\" width=\"77%\" />\n",
    "\n",
    "## 초모수(Hypter parameter) 튜닝\n",
    "\n",
    "XGBoost와 관련된 Hypter Parameter는 다음 세가지 범주로 나뉜다.  XGBoost가 의사결정나무에 기반하기 때문에 관련된 모수와 기계학습과 관련이 있기 때문에 이와 연관된 다수 초모수가 포함된다.\n",
    "\n",
    "- 학습 초모수\n",
    "    - `objective` [기본설정값=reg:linear]: 지도학습 손실 최소화 함수를 정의\n",
    "        - `binary:logistic`: 이항 분류 문제 로직스틱 회귀모형으로 반환값이 클래스가 아니라 예측 확률.\n",
    "        - `multi:softmax`: 다항 분류 문제의 경우 소프트맥스(Softmax)를 사용해서 분류하는데 반횐되는 값이 예측확률이 아니라 클래스임. 또한 `num_class`도 지정해야함.\n",
    "        - `multi:softprob`: 각 클래스 범주에 속하는 예측확률을 반환함.\n",
    "    - `eval_metric`: 설정한 `objective`별로 기본설정값이 지정되어 있음.\n",
    "        - rmse: root mean square error\n",
    "        - mae: mean absolute error\n",
    "        - logloss: negative log-likelihood\n",
    "        - error: Binary classification error rate (0.5 threshold)\n",
    "        - merror: Multiclass classification error rate\n",
    "        - mlogloss: Multiclass logloss\n",
    "        - auc: Area under the curve\n",
    "    - `seed` [기본설정값: 0]: 재현가능하도록 난수를 고정시킴.\n",
    "- 일반 초모수\n",
    "    - booster: 의사결정 기반 모형(`gbtree`), 선형 모형(`linear`)\n",
    "    - mthread: 병렬처리에 사용되는 코어수, 특정값을 지정하지 않는 경우 자동으로 시스템 코어수를 탐지하여 병렬처리에 동원함.\n",
    "- 부스팅 초모수\n",
    "    - `eta` [기본설정값: 0.3]: GBM에 학습율과 유사하고 일반적으로 0.01 ~ 0.2 값이 사용됨\n",
    "    - `min_child_weight` [기본설정값: 1]: 과적합(overfitting)을 방지할 목적으로 사용되는데, 너무 높은 값은 과소적합(underfitting)을 야기하기 때문에 CV를 사용해서 적절한 값이 제시되어야 한다.\n",
    "    - `max_depth` [기본설정값: 6]: 과적합 방지를 위해서 사용되는데 역시 CV를 사용해서 적절한 값이 제시되어야 하고 보통 3-10 사이 값이 적용된다.\n",
    "    - `max_leaf_nodes`: `max_leaf_nodes` 값이 설정되면 `max_depth`는 무시된다. 따라서 두값 중 하나를 사용한다.\n",
    "    - `max_delta_step` [기본설정값: 0]: 일반적으로 잘 사용되지 않음.\n",
    "    - `subsample` [기본설정값: 1]: 개별 의사결정나무 모형에 사용되는 임의 표본수를 지정. 보통 0.5 ~ 1 사용됨.\n",
    "    - `colsample_bytree` [기본설정값: 1]: 개별 의사결정나무 모형에 사용될 변수갯수를 지정. 보통 0.5 ~ 1 사용됨.\n",
    "    - `colsample_bylevel` [기본설정값: 1]: `subsample`, `colsample_bytree` 두 초모수 설정을 통해서 이미 의사결정나무 모형 개발에 사용될 변수갯수와 관측점 갯수를 사용했는데 추가로 `colsample_bylevel`을 지정하는 것이 특별한 의미를 갖는지 의문이 듦.\n",
    "    - `lambda` [기본설정값: 1]: 능선 회쉬(Ridge Regression)의 L2 정규화(regularization) 초모수. 그다지 많이 사용되고 있지는 않음.\n",
    "    - `alpha` [기본설정값: 0]: 라쏘 회귀(Lasso Regression)의 L1 정규화(regularization) 초모수로 차원이 높은 경우 알고리즘 속도를 높일 수 있음.\n",
    "    - `scale_pos_weight` [기본설정값: 1]: 클래스 불균형이 심한 경우 0보다 큰 값을 지정하여 효과를 볼 수 있음.\n",
    "\n",
    "\n",
    "참고문헌: [Analytics Vidhya, \"Complete Guide to Parameter Tuning in XGBoost with codes in Python\"](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost 초모수 확인\n",
    "\n",
    "XGBoost 초모수(Hyper Parameter)는 XGBoost 객체를 하나 생성한 후에 `print()` 함수로 확인이 가능하다. 그렇다고 모든 Hyper parameter가 중요한 것은 아니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "print(xgb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 초모수 헬로월드\n",
    "\n",
    "XGBoost 초모수를 간단한 헬로월드 코드를 작성해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.21.2\n",
      "xgboost version: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      3440\n",
      "           1       0.99      0.68      0.80      1060\n",
      "\n",
      "    accuracy                           0.92      4500\n",
      "   macro avg       0.95      0.84      0.88      4500\n",
      "weighted avg       0.93      0.92      0.92      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 경고 출력하지 않음 -----------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 라이브러리와 데이터 가져오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print('sklearn version: %s' % sklearn.__version__)\n",
    "print('xgboost version: %s' % xgb.__version__)\n",
    "\n",
    "hr_df = pd.read_csv(\"data/HR_comma_sep.csv\")\n",
    "hr_df.head()\n",
    "\n",
    "# 시험/훈련 데이터 관측점 구분\n",
    "colnames = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', \n",
    "       'promotion_last_5years']\n",
    "\n",
    "X, y = hr_df[colnames], hr_df[['left']]\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=777)\n",
    "\n",
    "# XGBoost 예측모형\n",
    "xgb_model = xgb.XGBClassifier(silent=False, \n",
    "                              booster='gbtree',\n",
    "                              scale_pos_weight=1,\n",
    "                              learning_rate=0.01,  \n",
    "                              colsample_bytree = 0.4,\n",
    "                              subsample = 0.8,\n",
    "                              objective='binary:logistic', \n",
    "                              n_estimators=100, \n",
    "                              max_depth=4, \n",
    "                              gamma=10, \n",
    "                              seed=777)\n",
    "\n",
    "hr_pred = xgb_model.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, hr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초모수 1개 대상 예측모형 구축\n",
    "\n",
    "`max_depth` 의사결정나무 모형 깊이 초모수를 달리해서 XGBoost 예측모형의 성능을 비교해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree depth</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.974889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.979333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.982222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.983556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.984889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree depth  accuracy\n",
       "0           3  0.974889\n",
       "1           5  0.979333\n",
       "2           7  0.982222\n",
       "3           9  0.983556\n",
       "4          10  0.984889"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "max_depth_list = [3,5,7,9,10]\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    xgb_model = xgb.XGBClassifier(max_depth=max_depth, seed=777)\n",
    "    xgb_pred = xgb_model.fit(X_train, y_train).predict(X_test)\n",
    "    xgb_accuracy = accuracy_score(y_test, xgb_pred) \n",
    "    accuracy_list.append(xgb_accuracy)\n",
    "    \n",
    "xgb_df = pd.DataFrame({'tree depth':max_depth_list, 'accuracy':accuracy_list})\n",
    "xgb_df.head()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
