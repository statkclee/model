source   <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[1]/span[1]') %>% html_text
text  <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[2]') %>%  html_text
news_df <- data.frame(title, source, text)
return(news_df)
}
page_list <- 1:10
keyword_list <- rep("데이터과학", 10)
ds_df <- map2_df(keyword_list, page_list, get_naver_news)
DT::datatable(ds_df)
# Chunk 3: naver-news-preprocessing
# KoSpacing 관련 팩키지 설치
# library(installr) # install.packages('installr')
# install.conda()
# reticulate::conda_version()
# reticulate::conda_list()
# devtools::install_github('haven-jeon/KoSpacing')
library(KoSpacing)
ds_df$text <- spacing(ds_df$text)
install_github("junhewk/RcppMeCab")
devtools::install_github("junhewk/RcppMeCab")
install.packages("RcppMeCab")
# install.packages("RcppMeCab")
library(RcppMeCab)
pos(ds_df$text, format = "data.frame")
pos(ds_df$text[1], format = "data.frame")
ds_df$text[1]
ds_df$text
ds_df$text <- spacing(ds_df$text) %>% unlist
ds_df$text[1]
pos(ds_df$text, format = "data.frame")
pos(ds_df$text[1], format = "data.frame")
ds_df$text[1]
ds_df$text[1] %>% class
pos(ds_df$text[1])
tmp_chr <- ds_df$text[1]
tmp_chr
tmp_chr <- ds_df$text[1]  %>% as.vector
tmp_chr
pos(tmp_chr)
pos(""양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다”고..."")
pos(""양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다”고...""
pos("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
pos("아버지가 방에 들어가신다")
? pos
sentence <- c(#some UTF-8 texts)
pos(sentence)
sentence <- c("아버지가 방에 들어가신다")
pos(sentence)
detach(RccMecabb)
devtools::install_github("junhewk/RmecabKo")
library(devtools)
assignInNamespace("version_info", c(devtools:::version_info, list("3.5" = list(version_min = "3.3.0", version_max = "99.99.99", path = "bin"))), "devtools")
find_rtools()
devtools::install_github("junhewk/RmecabKo")
pos("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
library(RmecabKo)
pos("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
install_mecab()
install_mecab("D:/Rlibs/mecab")
install_mecab("D:/Rlibs/mecab")
install_mecab("D:/Rlibs/mecab")
install_mecab("C:/Rlibs/mecab")
# Chunk 1
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
comment="", digits = 3, tidy = FALSE, prompt = TRUE, fig.align = 'center')
library(here)
# Chunk 2: naver-news-crawler
# 0. 환경설정 -----
library(tidyverse)
library(rvest)
library(glue)
# 1. 데이터 긁어오기 -----
## 네이버 뉴스 함수 작성
get_naver_news <- function(keyword, page){
url <- glue("https://search.naver.com/search.naver?&where=news&query=",keyword,"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=1&ds=2018.02.25&de=2018.03.04&docid=&nso=so:r,p:1w,a:all&mynews=0&cluster_rank=38&start=", page,"&refresh_start=0")
news_html <- read_html(url, handle = curl::new_handle("useragent" = "Mozilla/5.0"))
title     <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dt/a') %>% html_text
source   <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[1]/span[1]') %>% html_text
text  <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[2]') %>%  html_text
news_df <- data.frame(title, source, text)
return(news_df)
}
page_list <- 1:10
keyword_list <- rep("데이터과학", 10)
ds_df <- map2_df(keyword_list, page_list, get_naver_news)
DT::datatable(ds_df)
# Chunk 3: naver-news-preprocessing
# KoSpacing 관련 팩키지 설치
# library(installr) # install.packages('installr')
# install.conda()
# reticulate::conda_version()
# reticulate::conda_list()
# devtools::install_github('haven-jeon/KoSpacing')
library(KoSpacing)
ds_df$text <- spacing(ds_df$text) %>% unlist
library(RmecabKo)
install_mecab("C:/Rlibs/mecab")
install_mecab("D:/Rlibs/mecab")
tmp_chr <- ds_df$text[1]  %>% as.vector
pos("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
pos("Hello. This is R wrapper of Korean morpheme analyzer mecab-ko.")
tmp_chr <- ds_df$text[1]  %>% as.vector
tmp_chr
pos(tmp_chr)
? pos
pos(tmp_chr, join=FALSE)
pos_res <- pos(tmp_chr, join=FALSE)
pos_res %>% class
pos_res <- pos(tmp_chr, join=TRUE)
pos_res %>% class
pos_res
pos_res[[1]]
pos_res[[2]]
pos_res[[1]][1]
browseVignettes(RmecabKo)
help(package="RmecabKo"")
help(package="RmecabKo")
help(package="RmecabKo")
token_nouns(tmp_chr)
tmp_chr
token_nouns(pos_res)
token_nouns(pos_res[[1]])
ds_df$text[1]
token_nouns(ds_df$text[1])
token_words(ds_df$text[1])
token_morph(ds_df$text[1])
pos(ds_df$text[1])
token_morph(ds_df$text[1],  strip_punct = TRUE)
token_morph(ds_df$text[1],  strip_punct = TRUE, strip_numeric = TRUE)
words(ds_df$text[1])
ds_df$text[1]
words("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
nouns("양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다")
smpl_txt <- "양혜란 정책기획과장은 “높아진 시민들의 눈 높이에 맞춰 경험이나 직관이 아닌 데이터 기반의 과학적 행정이 필요하다. LH와의 협업을 통해 시민들이 체감할 수 있는 성과를 만들도록 노력하겠다"
token_ngrams(smpl_txt, n = 2)
token_ngrams(smpl_txt, n = 2)[[1]]
ds_df %>%
mutate(clean_text = str_replace_all(text, "[][!#$%()*,.:;<=>@^_`|~.{}]", ""))
ds_df %>%
mutate(clean_text = str_replace_all(text, "[[:punct:]]", ""))
ds_df <- ds_df %>%
mutate(clean_text = str_replace_all(text, "[[:punct:]]", ""))
ds_df %>%
mutate(text_pos = pos(clean_text))
ds_df <- ds_df %>%
mutate(text_pos = pos(clean_text))
View(ds_df)
ds_df %>%
unnest(text_pos)
ds_unnest_df <- ds_df %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG"))
ds_df
ds_df <- ds_df %>%
mutate(clean_text = str_replace_all(text, "[[:punct:]]", "")) %>%
tbl_df
ds_df <- ds_df %>%
mutate(text_pos = pos(clean_text))
ds_df
ds_noun_verb_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG|/VV"))
ds_noun_verb_df
ds_noun_verb_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "[/NNG|/VV]")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_noun_verb_df
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/\\b[NNG|/VV]\\b/]")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/\b[NNG|/VV]\b/]")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "[^/NNG$|$/VV$]]")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "[^/NNG$|$/VV$]")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "^/NNG$")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_noun_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG|/VV", ""))
ds_df <- ds_df %>%
mutate(text_pos = pos(clean_text))
ds_noun_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG", ""))
ds_noun_df %>%
count(source)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문"))
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
count(source, text_no_pos, sort=TRUE)
# Chunk 1
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
comment="", digits = 3, tidy = FALSE, prompt = TRUE, fig.align = 'center')
library(here)
# Chunk 2: naver-news-crawler
# 0. 환경설정 -----
library(tidyverse)
library(rvest)
library(glue)
# 1. 데이터 긁어오기 -----
## 네이버 뉴스 함수 작성
get_naver_news <- function(keyword, page){
url <- glue("https://search.naver.com/search.naver?&where=news&query=",keyword,"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=1&ds=2018.02.25&de=2018.03.04&docid=&nso=so:r,p:1w,a:all&mynews=0&cluster_rank=38&start=", page,"&refresh_start=0")
news_html <- read_html(url, handle = curl::new_handle("useragent" = "Mozilla/5.0"))
title     <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dt/a') %>% html_text
source   <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[1]/span[1]') %>% html_text
text  <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[2]') %>%  html_text
news_df <- data.frame(title, source, text)
return(news_df)
}
page_list <- 1:10
keyword_list <- rep("빅데이터", 10)
ds_df <- map2_df(keyword_list, page_list, get_naver_news)
DT::datatable(ds_df)
# Chunk 3: naver-news-preprocessing
# KoSpacing 관련 팩키지 설치
# library(installr) # install.packages('installr')
# install.conda()
# reticulate::conda_version()
# reticulate::conda_list()
# devtools::install_github('haven-jeon/KoSpacing')
library(KoSpacing)
ds_df$text <- spacing(ds_df$text) %>% unlist
# Chunk 5: naver-news-preprocessing-morpheme
# install.packages("RcppMeCab")
library(RmecabKo)
ds_df <- ds_df %>%
mutate(clean_text = str_replace_all(text, "[[:punct:]]", "")) %>%
tbl_df
# Chunk 6: naver-news-preprocessing-morpheme-pos
ds_df <- ds_df %>%
mutate(text_pos = pos(clean_text))
ds_noun_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG", ""))
ds_noun_df %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15, weight=n) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=fct_reorder(text_no_pos, n), y=n)) +
geom_col() +
theme_bw()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=fct_reorder(text_no_pos, n), y=n)) +
geom_col() +
theme_bw() +
coord_flip()
ds_df %>%
count(source)
ds_df %>%
count(source) %>%
arrange(desc(n))
ds_noun_df
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(차이 = abs(블록체인밸리 - 연합뉴스))
two_source_top_df <- ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(차이 = abs(블록체인밸리 - 연합뉴스)) %>%
top_n(15, 차이)
library(plotrix)
top_df <- ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(`차이` = abs(`블록체인밸리` - `연합뉴스`)) %>%
top_n(15, `차이`)
pyramid.plot(top_df$블록체인밸리, top_df$연합뉴스,
labels = top_df$차이,
gap = 12,
top.labels = c("블록체인밸리", "공통어", "연합뉴스"),
main = "빅데이터기사 공통사용 단어", unit = NULL)
top_df
pyramid.plot(top_df$블록체인밸리, top_df$연합뉴스,
labels = top_df$text_no_pos,
gap = 12,
top.labels = c("블록체인밸리", "공통어", "연합뉴스"),
main = "빅데이터기사 공통사용 단어", unit = NULL)
install.packages("dslabs")
install.packages("plotly")
install.packages("crosstalk")
install.packages("leaflet")
install.packages("gapminder")
install.packages("GGally")
library(reticulate)
conda_list()
use_condaenv("r-conda")
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
repl_python()
import numpy as np
exit
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
repl_python()
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
library(reticulate)
conda_list()
use_condaenv("r-conda")
use_condaenv("ANACON~1")
repl_python()
# 콘솔창 R에서 파이썬 변환
# reticulate::repl_python()
# KoNLPY 라이브러리
from konlpy.tag import Kkma, Hannanum, Komoran, Mecab, Twitter
# 형태소 분석기 생성자
hannanum = Hannanum()
# twitter = Twitter()
# mecab = Mecab()
# kkma = Kkma()
# komoran = Komoran()
# 문장을 형태소로 변환
morph_list = hannanum.morphs('@챗봇 내일 판매율 예측해서 Anderson한테 이메일로 보내줘.')
print(morph_list)
# 품사 POS(Part of Speech) 태그
pos_list = hannanum.pos('@챗봇 내일 판매율 예측해서 Anderson한테 이메일로 보내줘.')
print(pos_list)
# 단어추출
noun_list = hannanum.nouns('철학은 기술을 만들고 기술은 문화를 만든다')
print(noun_list)
library(babynames) # devtools::install_github("hadley/babynames")
babynames
babynames %>% tail
library(tidyverse)
babynames %>% tail
reticulate::repl_python()
repl_python()
reticulate::repl_python()
cheeses = ['Cheddar', 'Edam', 'Gouda']
cheeses.extend(['Mozzarella', 'Roquefort'])
print(cheeses)
cheeses.pop(position)
position = cheeses.index('Mozzarella')
# 치즈 리스트에서 해당 치즈 삭제
cheeses.pop(position)
print(cheeses)
quit
# Create the empty list: baby_names
baby_names = []
# Loop over records
for row in r$babynames:
# Add the name to the list
baby_names.append(row[3])
# Sort the names in alphabetical order
for name in sorted(baby_names):
# Print each name
print(name)
reticulate::repl_python()
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
reticulate::repl_python()
print('hll')
baby_names = []
baby_names = []
for row in r$babynames:
quit
for row in r$babynames:
# Add the name to the list
baby_names.append(row[3])
# Sort the names in alphabetical order
for name in sorted(baby_names):
# Print each name
print(name)
reticulate::repl_python()
babynames
babynames
quit
library(tidyverse)
library(babynames) # devtools::install_github("hadley/babynames")
reticulate::repl_python()
r$babynames
setwd("D:/docs/model")
setwd("C:/Users/victor/Dropbox/02_model/explainable_cloudera")
setwd("D:/docs/model")
install.packages("texreg")
